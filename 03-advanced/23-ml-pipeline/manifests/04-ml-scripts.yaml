apiVersion: v1
kind: ConfigMap
metadata:
  name: ml-scripts
  namespace: ml-pipeline
  labels:
    app: ml-pipeline
data:
  preprocess_data.py: |
    #!/usr/bin/env python3
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    import os
    
    print("üîÑ Starting data preprocessing...")
    
    # Generate sample data (in real scenario, this would load from external sources)
    np.random.seed(42)
    n_samples = 10000
    
    # Generate features
    X = np.random.randn(n_samples, 4)
    X[:, 0] = X[:, 0] * 2 + 1  # feature scaling
    X[:, 1] = X[:, 1] * 0.5 + 2
    
    # Generate target with some relationship to features
    y = (X[:, 0] * 0.5 + X[:, 1] * 0.3 + X[:, 2] * 0.2 + np.random.randn(n_samples) * 0.1) > 0
    y = y.astype(int)
    
    # Create DataFrame
    df = pd.DataFrame(X, columns=['feature_1', 'feature_2', 'feature_3', 'feature_4'])
    df['target'] = y
    
    print(f"üìä Generated dataset shape: {df.shape}")
    print(f"üìà Target distribution: {df['target'].value_counts().to_dict()}")
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        df.drop('target', axis=1), df['target'], 
        test_size=0.2, random_state=42, stratify=df['target']
    )
    
    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Save processed data
    os.makedirs('/data/processed', exist_ok=True)
    
    np.save('/data/processed/X_train.npy', X_train_scaled)
    np.save('/data/processed/X_test.npy', X_test_scaled)
    np.save('/data/processed/y_train.npy', y_train.values)
    np.save('/data/processed/y_test.npy', y_test.values)
    
    # Save scaler for inference
    import pickle
    with open('/data/processed/scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    
    print("‚úÖ Data preprocessing completed!")
    print(f"üìÅ Training set: {X_train_scaled.shape}")
    print(f"üìÅ Test set: {X_test_scaled.shape}")
    
  train_model.py: |
    #!/usr/bin/env python3
    import numpy as np
    import tensorflow as tf
    from sklearn.metrics import classification_report, accuracy_score
    import os
    
    print("üöÄ Starting model training...")
    
    # Load preprocessed data
    X_train = np.load('/data/processed/X_train.npy')
    X_test = np.load('/data/processed/X_test.npy')
    y_train = np.load('/data/processed/y_train.npy')
    y_test = np.load('/data/processed/y_test.npy')
    
    print(f"üìä Training data shape: {X_train.shape}")
    print(f"üìä Test data shape: {X_test.shape}")
    
    # Build model
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(16, activation='relu'),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    
    # Compile model
    model.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    print("üèóÔ∏è Model architecture:")
    model.summary()
    
    # Train model
    history = model.fit(
        X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.2,
        verbose=1
    )
    
    # Evaluate model
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"üìà Test Accuracy: {test_accuracy:.4f}")
    
    # Make predictions for detailed evaluation
    y_pred_proba = model.predict(X_test)
    y_pred = (y_pred_proba > 0.5).astype(int).flatten()
    
    print("üìä Classification Report:")
    print(classification_report(y_test, y_pred))
    
    # Save model in TensorFlow SavedModel format
    model_path = '/models/my_model/1'
    os.makedirs(model_path, exist_ok=True)
    model.save(model_path)
    
    # Save training history
    import json
    history_dict = {
        'loss': [float(x) for x in history.history['loss']],
        'accuracy': [float(x) for x in history.history['accuracy']],
        'val_loss': [float(x) for x in history.history['val_loss']],
        'val_accuracy': [float(x) for x in history.history['val_accuracy']]
    }
    
    with open('/models/training_history.json', 'w') as f:
        json.dump(history_dict, f)
    
    print("‚úÖ Model training completed!")
    print(f"üìÅ Model saved to: {model_path}")
    print(f"üéØ Final test accuracy: {test_accuracy:.4f}")
    
  evaluate_model.py: |
    #!/usr/bin/env python3
    import numpy as np
    import requests
    import json
    import time
    
    print("üß™ Starting model evaluation...")
    
    # Load test data
    X_test = np.load('/data/processed/X_test.npy')
    y_test = np.load('/data/processed/y_test.npy')
    
    # Model serving endpoint
    model_url = "http://model-serving-service:8501/v1/models/my_model:predict"
    
    # Test with a few samples
    test_samples = X_test[:10]
    actual_labels = y_test[:10]
    
    print("üîç Testing model serving endpoint...")
    
    for i, sample in enumerate(test_samples):
        data = {
            "instances": [sample.tolist()]
        }
        
        try:
            response = requests.post(model_url, json=data, timeout=10)
            if response.status_code == 200:
                prediction = response.json()['predictions'][0][0]
                predicted_class = 1 if prediction > 0.5 else 0
                actual_class = actual_labels[i]
                
                print(f"Sample {i+1}: Predicted={predicted_class} (prob={prediction:.3f}), Actual={actual_class}")
            else:
                print(f"‚ùå Error for sample {i+1}: {response.status_code}")
        except Exception as e:
            print(f"‚ùå Request failed for sample {i+1}: {e}")
        
        time.sleep(0.1)  # Small delay between requests
    
    print("‚úÖ Model evaluation completed!")
